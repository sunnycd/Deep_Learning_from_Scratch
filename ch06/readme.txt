本章讲述最原始 DNN 进化成现在 DNN ，解决了那些问题以及 DNN 训练技巧
 1、梯度下降算法：SGD --> Adam 进化之路，但 Adam 并不完美：https://zhuanlan.zhihu.com/p/32338983
 2、权重初始化：
        Xavier：适用 sigmoid；
        He：适用 ReLU
 3、Batch Normalization：解决权重初始化设置不当产生输出分布异常问题
 4、过拟合：
        L2 正则：惩罚模型
        Dropout：随机丢弃神经元，集成方法思路
 5、超参优化：贝叶斯最优化
    1、数据集分为训练数据、验证数据、测试数据：训练数据训练模型，验证数据评估模型性能，测试数据模型泛化能力(测试数据只在最后仅使用一次)
    2、最优超参寻找：首先在大范围搜索，然后逐渐减小范围

 问题：
    训练时，batch 样本的选择，有放回取样还是无放回取样
